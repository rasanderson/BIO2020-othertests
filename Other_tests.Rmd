---
title: "Other statistical tests"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(mosaic)
library(ggformula)
library(patchwork)
library(sm)
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# Functions from example site
# Generate normal data with known parameters
rnorm_fixed = function(N, mu = 0, sd = 1)
  scale(rnorm(N)) * sd + mu
signed_rank = function(x) sign(x) * rank(abs(x))
# Plot style.
theme_axis = function(P,
                      jitter = FALSE,
                      xlim = c(-0.5, 2),
                      ylim = c(-0.5, 2),
                      legend.position = NULL) {
  P = P + theme_bw(15) +
    geom_segment(
      x = -1000, xend = 1000,
      y = 0, yend = 0,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    geom_segment(
      x = 0, xend = 0,
      y = -1000, yend = 1000,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    coord_cartesian(xlim = xlim, ylim = ylim) +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      legend.position = legend.position
    )
  
  # Return jittered or non-jittered plot?
  if (jitter) {
    P + geom_jitter(width = 0.1, size = 2)
  }
  else {
    P + geom_point(size = 2)
  }
}

signed_rank = function(x) sign(x) * rank(abs(x))

```


```{r, echo=FALSE}
set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)
```


## "Please do stats test XYZ"

When you are collecting data for other modules, or in your Stage 3 projects, you
will often find that a member of staff is suggesting that you analyse your data
with a statistical test that you have not heard of. However, many of these 
'standard' statistical tests can be considered to be linear models. If you find
it less confusing to consider them as forms of linear models, then feel free to
stick to that. If you prefer to use the built-in R commands for each test, then
that is fine also. **What matters is the underlying hypothesis** that you are
trying to test. This will determine the exact structure of your `lm` or `glm`
function, or which built-in R command to use instead.

This website simply compares some of the commonly encounted statistical tests
in their __linear model__ format and their __built-in__ format. You can find
full details of this philosphy for teaching statistics at https://lindeloev.github.io/tests-as-linear/

### t-distribution
A lot of 'standard' statistical tests use the t-distribution, and you will often
here people refer to 't-tests', 'paired t-tests', 'one-sample t-tests'. These are
generally introduced early in statistics courses, as they are all relatively 
simple tests, although you have to know when to apply which one. A disadvantage is
that in Stage 3, when I ask students **"How are you planning to analyse your data?"**
the most common answer is **"Oh, perhaps a t-test"** because this is the first 
stats test they learnt and hence the easiest to remember. But first a quiz about
drinks (which yes, does link to t-tests)...

```{r quiz}
question("Which alcoholic beverage is related to the t-distribution?",
         answer("Smirnoff vodka"),
         answer("Rioja wine"),
         answer("Guiness stout", correct = TRUE),
         answer("Newcastle Brown Ale"),
         answer("Jameson's whiskey"),
         allow_retry = TRUE
)
```

Even if you managed to find the correct answer, you're probably puzzled (unless
you already know the strange story). Keep reading...

## The t-distribution
This distribution looks very similar to a normal distribution in that it forms a
bell-shaped curve. The differences are only obvious when one is laid on top of
the other, with the t-distribution in red:

```{r t_vs_normal}
xvs <- seq(-4, 4, 0.01)
plot(xvs, dnorm(xvs), type="l", lty=2, ylab="Probability density", xlab="Deviates")
lines(xvs, dt(xvs, df=5), col="red")
```

The two curves are very similar, with the t-distribution having "fatter" tails. It
is generall known as **Student's t-distribution**. It was developed by W.S.Gossett
in 1908 who worked for Guiness Breweries in Dublin

![Guinness and t-distribution](www/Gosset.jpg)

Gossett wanted to distinguish between the quality of Guiness stout based on the
different types of malt used in the brewing process, and developed the t-distribution
instead of the normal distribution as it was slightly more robust with small sample
sizes. However, Guiness Breweries wanted to keep the method secret, and would not
let him publish the technique. Gossett then published it under the pseudonym "Student"
and the name has stuck.

You will often encounter t-statistics, often to check whether an estimated parameter
from a model, such as an intercept or gradient, differs from zero. If the parameter
is big (large negative or positive value), then the t-statistic will statistically
significant (p<0.05), whereas if the parameter is roughly zero, the t-statistic will
be non-significant (p>0.05). You may encounter several different types of t-tests,
as well as their **non-parametric** equivalents.

### What is a non-parametric test?
Many statistical tests exist in non-parametric versions, including t-tests, and
you may be asked about them from project supervisor or encounter them in textbooks.
They work on **rankings** rather than the original raw data. So they would convert
the response variable numbers `5.2, 1.8, 8.9` into `2, 3, 1` to represent their
**order**. They can be useful when your data are very badly skewed, but they are
far less powerful in detecting patterns.

### Theory: rank-transformation

The R command `rank` simply takes a list of numbers and "replaces" them with the
integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the
result of the rank-transformation `rank(c(3.6, 3.4, -5.0, 8.2))` is `3, 2, 1, 4`.

A *signed* rank is the same, just where we rank according to absolute size first
and then add in the sign second. So the signed rank here would be `2, 1, -3, 4`.
Or in code:

```{r}
signed_rank = function(x) sign(x) * rank(abs(x))
```

I hope I don't offend anyone when I say that ranks are easy; yet it's all you need to do to convert most parametric tests into their "non-parametric" counterparts! One interesting implication is that *many "non-parametric tests" are about as parametric as their parametric counterparts with means, standard deviations, homogeneity of variance, etc. - just on rank-transformed data*. That's why I put "non-parametric" in quotation marks.

## One-sample t-test
This is the simplest form of t-test. You are basically trying to determine whether
your numbers differ from zero (either positively or negatively), or whether your
dataset is roughly the same as zero. The non-parametric equivalent is known as
the Wilcoxon signed-rank test. **Note** As all you are trying to do is determine
whether a set of numbers differs from zero, there is not an explanatory variable.

The following diagram summarises the idea for the one-sample t-test and Wilcoxon
test respectively. The question you are addressing is whether the blue horizontal
line significantly differs from zero.

```{r one_sample_t_test, warning=FALSE}


# T-test
D_t1 = data.frame(y = rnorm_fixed(20, 0.5, 0.6),
                  x = runif(20, 0.93, 1.07))  # Fix mean and SD

P_t1 = ggplot(D_t1, aes(y = y, x = 0)) + 
  stat_summary(fun.y=mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y.., color='beta_0'), lwd=2) +
  scale_color_manual(name = NULL, values = c("blue"), labels = c(bquote(beta[0] * " (intercept)"))) +
  
  geom_text(aes(label = round(y, 1)), nudge_x = 0.2, size = 3, color = 'dark gray') + 
  labs(title='         T-test')

# Wilcoxon
D_t1_rank = data.frame(y = signed_rank(D_t1$y))

P_t1_rank = ggplot(D_t1_rank, aes(y = y, x = 0)) + 
  stat_summary(fun.y = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..,  color = 'beta_0'), lwd = 2) +
  scale_color_manual(name = NULL, values = c("blue"), labels = c(bquote(beta[0] * " (intercept)"))) +

  geom_text(aes(label = y), nudge_x = 0.2, size = 3, color = 'dark gray') + 
  labs(title='         Wilcoxon')


# Stich together using patchwork
theme_axis(P_t1, ylim = c(-1, 2), legend.position = c(0.6, 0.1)) + 
  theme_axis(P_t1_rank, ylim = NULL,  legend.position = c(0.6, 0.1))
```

In the above plot, notice how the original values are showing for the t.test
plot, whereas the ranked (ordered) values are showing for the non-parametric
Wilcoson plot.

#### One-sample t-test
This is how you can check whether your mean value for your data is different from
zero, using the conventional built-in `t.test()` function, or when expressed
as a linear model with `lm()`. Notice for the linear model, we are setting the
explanatory variable to the number `1` as we simply want to check if we differ
from zero.

Let's try the two functions on a dataset `y` with 50 values (you will see 49
degrees of freedom as a result)

```{r, one_samp_t-setup}
set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

```

```{r one_samp_t, exercise=TRUE}
# Built-in t-test
a <- t.test(y)
a

# Equivalent linear model: intercept-only
b <- lm(y ~ 1)
summary(b)
```

Although the formatting of the output from the two tests is slightly different
note that the mean (shown as *Estimate* for `lm`) t-statistics, p-values etc.
are almost identical.

### Wilcoxon test
This is the non-parametric (rank-based) equivalent. Here we use a signed-rank
method. We can use the `wilcoxon.test()` or `t.test()` functions, but these are
yet more functions to remember. Probably easier to think of it as a standard
linear model, use `lm()` and add the `signed_rank()` function. The following all
give the same results:

```{r, wilcoxon-setup}
set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

```

```{r wilcoxon, exercise=TRUE}
# a) Built-in
wilcox.test(y)

# b) Equivalent linear model
b = lm(signed_rank(y) ~ 1)  # See? Same model as above, just on signed ranks
summary(b)

# c) Bonus: of course also works for one-sample t-test
t.test(signed_rank(y))
```

The p-statistics are approximately the same for all three methods. With small
sample sizes the `wilcox.test()` will be slightly more accurate.

## Paired t-tests
Sometimes you will have a set of samples that are coupled together in some way.
For example:

* You collect water samples upstream and downstream from sewage outlets from
30 different rivers, and test for the bacterial diversity in the samples. Now the
bacterial biodiversity may well differ between your individual rivers. What you
are really interested in is the **difference** in diversity upstream and downstream.
So you first calculate the difference between the bacterial diversity, and then
undertake a `t.test` or `lm` on the resulting difference.

```{r paired_t_plot}
# Data for plot
N = nrow(D_t1)
start = rnorm_fixed(N, 0.2, 0.3)
D_tpaired = data.frame(
  x = rep(c(0, 1), each = N),
  y = c(start, start + D_t1$y),
  id = 1:N
)

# Plot
P_tpaired = ggplot(D_tpaired, aes(x = x, y = y)) +
  geom_line(aes(group = id)) +
  labs(title = '          Pairs')

# Use patchwork to put them side-by-side
theme_axis(P_tpaired) + theme_axis(P_t1, legend.position = c(0.6, 0.1))
```

The plot on the left shows the original samples, with the pairs (e.g. upstream or
downstream of a sewer outflow) linked by lines. The plot above right shows the
results of subtracting each pair from the other, that will be analysed to see 
if the average of these subtractions is significantly different from zero.

Here is an example of the analysis in R, using either the `t.test()` or `lm()`
functions on a dataset of 50 paired values, the mean difference in the pairs 
being -0.595. Is this value significantly different from zero?

```{r paired_t_test_analysis, exercise=TRUE}
t.test(y, y2, paired = TRUE) # Built-in paired t-test; note use of paired = TRUE

b = lm(y - y2 ~ 1) # Equivalent linear model. Simply subtract the pairs
summary(b)
```

Again you can see that the results from the two analyses are very similar. Note
that the formula for the `lm()` function is of the structure

$$Response \text{ ~ 1}$$
where

* $Response$ is $y - y2$ to show that the **differences** are being studied
* $Explanatory$ is simply $1$ i.e. the mean (intercept in `lm`): are these differences the same as zero?

### Wilcoxon matched-pairs
This is the non-parametric equivalent of the paired t-test. We can use the same
trick as before, of subtracting the values for one pair from the other in the
`lm()` function, but also using the `signed_rank()` function:

```{r, wilcoxon-paired, exercise=TRUE}
# a. Built-in Wilcoxon matched pairs
wilcox.test(y, y2, paired = TRUE)

# b. Equivalent linear model:
b <- lm(signed_rank(y - y2) ~ 1)
summary(b)

# c. Bonus: identical to one-sample t-test one signed ranks
t.test(signed_rank(y - y2))
```

Again, you can see that the p-values are fairly similar. Personally, I think that
sticking to the `lm()` formulation is much simpler conceptually.



