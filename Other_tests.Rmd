---
title: "Other statistical tests"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(mosaic)
library(ggformula)
library(patchwork)
library(sm)
library(bio2020)
knitr::opts_chunk$set(echo = FALSE)

# Functions from example site
# Generate normal data with known parameters
rnorm_fixed = function(N, mu = 0, sd = 1)
  scale(rnorm(N)) * sd + mu
signed_rank = function(x) sign(x) * rank(abs(x))
# Plot style.
theme_axis = function(P,
                      jitter = FALSE,
                      xlim = c(-0.5, 2),
                      ylim = c(-0.5, 2),
                      legend.position = NULL) {
  P = P + theme_bw(15) +
    geom_segment(
      x = -1000, xend = 1000,
      y = 0, yend = 0,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    geom_segment(
      x = 0, xend = 0,
      y = -1000, yend = 1000,
      lty = 2, color = 'dark gray', lwd = 0.5
    ) +
    coord_cartesian(xlim = xlim, ylim = ylim) +
    theme(
      axis.title = element_blank(),
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.border = element_blank(),
      panel.grid = element_blank(),
      legend.position = legend.position
    )
  
  # Return jittered or non-jittered plot?
  if (jitter) {
    P + geom_jitter(width = 0.1, size = 2)
  }
  else {
    P + geom_point(size = 2)
  }
}

signed_rank = function(x) sign(x) * rank(abs(x))


set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)
```


## "Please do stats test XYZ"

When you are collecting data for other modules, or in your Stage 3 projects, you
will often find that a member of staff is suggesting that you analyse your data
with a statistical test that you have not heard of. However, many of these 
'standard' statistical tests can be considered to be linear models. If you find
it less confusing to consider them as forms of linear models, then feel free to
stick to that. If you prefer to use the built-in R commands for each test, then
that is fine also. **What matters is the underlying hypothesis** that you are
trying to test. This will determine the exact structure of your `lm` or `glm`
function, or which built-in R command to use instead.

This website simply compares some of the commonly encounted statistical tests
in their __linear model__ format and their __built-in__ format. You can find
full details of this philosphy for teaching statistics at https://lindeloev.github.io/tests-as-linear/

### t-distribution
A lot of 'standard' statistical tests use the t-distribution, and you will often
here people refer to 't-tests', 'paired t-tests', 'one-sample t-tests'. These are
generally introduced early in statistics courses, as they are all relatively 
simple tests, although you have to know when to apply which one. A disadvantage is
that in Stage 3, when I ask students **"How are you planning to analyse your data?"**
the most common answer is **"Oh, perhaps a t-test"** because this is the first 
stats test they learnt and hence the easiest to remember. But first a quiz about
drinks (which yes, does link to t-tests)...

```{r quiz}
question("Which alcoholic beverage is related to the t-distribution?",
         answer("Smirnoff vodka"),
         answer("Rioja wine"),
         answer("Guiness stout", correct = TRUE),
         answer("Newcastle Brown Ale"),
         answer("Jameson's whiskey"),
         allow_retry = TRUE
)
```

Even if you managed to find the correct answer, you're probably puzzled (unless
you already know the strange story). Keep reading...

## The t-distribution
This distribution looks very similar to a normal distribution in that it forms a
bell-shaped curve. The differences are only obvious when one is laid on top of
the other, with the t-distribution in red:

```{r t_vs_normal}
xvs <- seq(-4, 4, 0.01)
plot(xvs, dnorm(xvs), type="l", lty=2, ylab="Probability density", xlab="Deviates")
lines(xvs, dt(xvs, df=5), col="red")
```

The two curves are very similar, with the t-distribution having "fatter" tails. It
is generall known as **Student's t-distribution**. It was developed by W.S.Gossett
in 1908 who worked for Guiness Breweries in Dublin

![Guinness and t-distribution](www/Gosset.jpg)

Gossett wanted to distinguish between the quality of Guiness stout based on the
different types of malt used in the brewing process, and developed the t-distribution
instead of the normal distribution as it was slightly more robust with small sample
sizes. However, Guiness Breweries wanted to keep the method secret, and would not
let him publish the technique. Gossett then published it under the pseudonym "Student"
and the name has stuck.

You will often encounter t-statistics, often to check whether an estimated parameter
from a model, such as an intercept or gradient, differs from zero. If the parameter
is big (large negative or positive value), then the t-statistic will statistically
significant (p<0.05), whereas if the parameter is roughly zero, the t-statistic will
be non-significant (p>0.05). You may encounter several different types of t-tests,
as well as their **non-parametric** equivalents.

### What is a non-parametric test?
Many statistical tests exist in non-parametric versions, including t-tests, and
you may be asked about them from project supervisor or encounter them in textbooks.
They work on **rankings** rather than the original raw data. So they would convert
the response variable numbers `5.2, 1.8, 8.9` into `2, 3, 1` to represent their
**order**. They can be useful when your data are very badly skewed, but they are
far less powerful in detecting patterns.

### Theory: rank-transformation

The R command `rank` simply takes a list of numbers and "replaces" them with the
integers of their rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the
result of the rank-transformation `rank(c(3.6, 3.4, -5.0, 8.2))` is `3, 2, 1, 4`.

A *signed* rank is the same, just where we rank according to absolute size first
and then add in the sign second. So the signed rank here would be `2, 1, -3, 4`.
Or in code:

```{r}
signed_rank = function(x) sign(x) * rank(abs(x))
```

I hope I don't offend anyone when I say that ranks are easy; yet it's all you need to do to convert most parametric tests into their "non-parametric" counterparts! One interesting implication is that *many "non-parametric tests" are about as parametric as their parametric counterparts with means, standard deviations, homogeneity of variance, etc. - just on rank-transformed data*. That's why I put "non-parametric" in quotation marks.

## One-sample t-test
This is the simplest form of t-test. You are basically trying to determine whether
your numbers differ from zero (either positively or negatively), or whether your
dataset is roughly the same as zero. The non-parametric equivalent is known as
the Wilcoxon signed-rank test. **Note** As all you are trying to do is determine
whether a set of numbers differs from zero, there is not an explanatory variable.

The following diagram summarises the idea for the one-sample t-test and Wilcoxon
test respectively. The question you are addressing is whether the blue horizontal
line significantly differs from zero.

```{r one_sample_t_test, warning=FALSE}


# T-test
D_t1 = data.frame(y = rnorm_fixed(20, 0.5, 0.6),
                  x = runif(20, 0.93, 1.07))  # Fix mean and SD

P_t1 = ggplot(D_t1, aes(y = y, x = 0)) + 
  stat_summary(fun.y=mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y.., color='beta_0'), lwd=2) +
  scale_color_manual(name = NULL, values = c("blue"), labels = c(bquote(beta[0] * " (intercept)"))) +
  
  geom_text(aes(label = round(y, 1)), nudge_x = 0.2, size = 3, color = 'dark gray') + 
  labs(title='         T-test')

# Wilcoxon
D_t1_rank = data.frame(y = signed_rank(D_t1$y))

P_t1_rank = ggplot(D_t1_rank, aes(y = y, x = 0)) + 
  stat_summary(fun.y = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..,  color = 'beta_0'), lwd = 2) +
  scale_color_manual(name = NULL, values = c("blue"), labels = c(bquote(beta[0] * " (intercept)"))) +

  geom_text(aes(label = y), nudge_x = 0.2, size = 3, color = 'dark gray') + 
  labs(title='         Wilcoxon')


# Stich together using patchwork
theme_axis(P_t1, ylim = c(-1, 2), legend.position = c(0.6, 0.1)) + 
  theme_axis(P_t1_rank, ylim = NULL,  legend.position = c(0.6, 0.1))
```

In the above plot, notice how the original values are showing for the t.test
plot, whereas the ranked (ordered) values are showing for the non-parametric
Wilcoson plot.

#### One-sample t-test
This is how you can check whether your mean value for your data is different from
zero, using the conventional built-in `t.test()` function, or when expressed
as a linear model with `lm()`. Notice for the linear model, we are setting the
explanatory variable to the number `1` as we simply want to check if we differ
from zero.

Let's try the two functions on a dataset `y` with 50 values (you will see 49
degrees of freedom as a result)

```{r, one_samp_t-setup}
set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

```

```{r one_samp_t, exercise=TRUE}
# Built-in t-test
a <- t.test(y)
a

# Equivalent linear model: intercept-only
b <- lm(y ~ 1)
summary(b)
```

Although the formatting of the output from the two tests is slightly different
note that the mean (shown as *Estimate* for `lm`) t-statistics, p-values etc.
are almost identical.

### Wilcoxon test
This is the non-parametric (rank-based) equivalent. Here we use a signed-rank
method. We can use the `wilcoxon.test()` or `t.test()` functions, but these are
yet more functions to remember. Probably easier to think of it as a standard
linear model, use `lm()` and add the `signed_rank()` function. The following all
give the same results:

```{r, wilcoxon-setup}
set.seed(40)
# Data from example site
y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0))  # Almost zero mean, not normal
x = rnorm_fixed(50, mu = 0, sd = 1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

```

```{r wilcoxon, exercise=TRUE}
# a) Built-in
wilcox.test(y)

# b) Equivalent linear model
b = lm(signed_rank(y) ~ 1)  # See? Same model as above, just on signed ranks
summary(b)

# c) Bonus: of course also works for one-sample t-test
t.test(signed_rank(y))
```

The p-statistics are approximately the same for all three methods. With small
sample sizes the `wilcox.test()` will be slightly more accurate.

## Paired t-tests
Sometimes you will have a set of samples that are coupled together in some way.
For example:

* You collect water samples upstream and downstream from sewage outlets from
30 different rivers, and test for the bacterial diversity in the samples. Now the
bacterial biodiversity may well differ between your individual rivers. What you
are really interested in is the **difference** in diversity upstream and downstream.
So you first calculate the difference between the bacterial diversity, and then
undertake a `t.test` or `lm` on the resulting difference.

```{r paired_t_plot}
# Data for plot
N = nrow(D_t1)
start = rnorm_fixed(N, 0.2, 0.3)
D_tpaired = data.frame(
  x = rep(c(0, 1), each = N),
  y = c(start, start + D_t1$y),
  id = 1:N
)

# Plot
P_tpaired = ggplot(D_tpaired, aes(x = x, y = y)) +
  geom_line(aes(group = id)) +
  labs(title = '          Pairs')

# Use patchwork to put them side-by-side
theme_axis(P_tpaired) + theme_axis(P_t1, legend.position = c(0.6, 0.1))
```

The plot on the left shows the original samples, with the pairs (e.g. upstream or
downstream of a sewer outflow) linked by lines. The plot above right shows the
results of subtracting each pair from the other, that will be analysed to see 
if the average of these subtractions is significantly different from zero.

Here is an example of the analysis in R, using either the `t.test()` or `lm()`
functions on a dataset of 50 paired values, the mean difference in the pairs 
being -0.595. Is this value significantly different from zero?

```{r paired_t_test_analysis, exercise=TRUE}
t.test(y, y2, paired = TRUE) # Built-in paired t-test; note use of paired = TRUE

b = lm(y - y2 ~ 1) # Equivalent linear model. Simply subtract the pairs
summary(b)
```

Again you can see that the results from the two analyses are very similar. Note
that the formula for the `lm()` function is of the structure

$$Response \text{ ~ 1}$$
where

* $Response$ is $y - y2$ to show that the **differences** are being studied
* $Explanatory$ is simply $1$ i.e. the mean (intercept in `lm`): are these differences the same as zero?

### Wilcoxon matched-pairs
This is the non-parametric equivalent of the paired t-test. We can use the same
trick as before, of subtracting the values for one pair from the other in the
`lm()` function, but also using the `signed_rank()` function:

```{r, wilcoxon-paired, exercise=TRUE}
# a. Built-in Wilcoxon matched pairs
wilcox.test(y, y2, paired = TRUE)

# b. Equivalent linear model:
b <- lm(signed_rank(y - y2) ~ 1)
summary(b)

# c. Bonus: identical to one-sample t-test one signed ranks
t.test(signed_rank(y - y2))
```

Again, you can see that the p-values are fairly similar. Personally, I think that
sticking to the `lm()` formulation is much simpler conceptually.

## Independent t-test
This is the basic t-test, sometimes referred to as "Student's t-test". Basically
you are trying to compare two means to see if they are different:

```{r, warning=FALSE}
# Data
N = 20  # Number of data points per group
D_t2 = data.frame(
  x = rep(c(0, 1), each=N),
  y = c(rnorm_fixed(N, 0.3, 0.3), rnorm_fixed(N, 1.3, 0.3))
)

# Plot
P_t2 = ggplot(D_t2, aes(x=x, y=y)) + 
  stat_summary(fun.y = mean, geom = "errorbar", aes(ymax = ..y.., ymin = ..y..,  color = 'something'), lwd = 2) +
  geom_segment(x = -10, xend = 10, y = 0.3, yend = 0.3, lwd = 2, aes(color = 'beta_0')) + 
  geom_segment(x = 0, xend = 1, y = 0.3, yend = 1.3, lwd = 2, aes(color = 'beta_1')) + 
  
  scale_color_manual(name = NULL, values = c("blue", "red", "darkblue"), labels=c(bquote(beta[0]*" (group 1 mean)"), bquote(beta[1]*" (slope = difference)"), bquote(beta[0]+beta[1]%.%1*" (group 2 mean)")))
  #scale_x_discrete(breaks=c(0.5, 1.5), labels=c('1', '2'))

theme_axis(P_t2, jitter = TRUE, xlim = c(-0.3, 2), legend.position = c(0.53, 0.08))
```

You can use the `t.test()` function or the `lm()` function. For the latter, it
is easier if your data are in long format, i.e. the response in one column,
and an explanatory **factor** for the two means in the other. If they are in
two columns, a little bit more work is needed.

```{r, student_t_test-setup}
y_values <- c(y, y2)
group_codes <- c(rep("site1", 50), rep("site2", 50))
t_data <- data.frame(y_values = y_values, group_codes=group_codes)
```


```{r, student_t_test, exercise=TRUE}
# a. Built-in independent t-test on wide data 
# data from y and y2 are two different columns, y and y2
t.test(y, y2, var.equal = TRUE)

# b. For lm it is easiest if there is one longer column for all the response
# data, and a single explanatory giving a code for which level is being compared
# Here the explanatory is called group_codes, with 2 sites, but it could just as
# easily be 2 laboratory treatments.
print(t_data)

# Simply use lm in the usual way
b <- lm(y_values ~ group_codes, data=t_data)
summary(b)

```


The p-values and t-statistics are roughly the same for the two functions, but again
I think that sticking to the `lm()` structure gives greater consistency and
clarity than `t.test()`. In the `lm()` output note that:

* The `Estimate` on the row marked `Intercept` is actually the mean value for the
first level of your explanatory, i.e. `site1`
* The `Estimate` for `group_codessite2` is the **difference** in the mean value of
the two levels of your explanatory, which is significant
* The absolute value of the t-statistic is the same for either the `lm()` or `t.test()` methods

## Correlation
You're probably familiar with correlation already. High correlation shows a strong
relationship between two variables, and low correlation occurs when there is a lot
of scatter

```{r correlation_plot, warning=FALSE}
set.seed(40)
var1 <- rnorm(30, 15, 0.5)
var2 <- var1 * 8 + rnorm(30, 2, 1.51)
var3 <- var1 * 8 + rnorm(30, 2, 6)

p1 <- gf_point(var2 ~ var1, xlab = "variable 1", ylab="variable 2",
               title = paste("Correlation =", round(cor(var1, var2), 3))) %>% 
  gf_lm(var2 ~ var1) %>% 
  gf_lims(x = c(14, 16), y = c(105, 135))

 
p2 <- gf_point(var3 ~ var1, xlab = "variable 1", ylab="variable 3",
               title = paste("Correlation =", round(cor(var1, var3), 3))) %>% 
  gf_lm(var3 ~ var1) %>% 
  gf_lims(x = c(14, 16), y = c(105, 135)) 

multi_plot(p1, p2, cols=2)
```

You can see that the correlation is lower on the plot on the right, where the
scatter around the line is much bigger. You can calculate the correlation
coefficient with the `cor()` function. Notice that **correlation does not
imply causality**. Although the plots above show _variable 1_ on the x-axis and
_variable 2_ on the y-axis, they could just as easily be the other way round.
Note also that the gradients of the two lines are roughly the same, but that
does not affect the correlation.

```{r, correlation_test, exercise=TRUE}
# Simple correlation coefficient
cor(var3, var1)

#  cor.test function
cor.test(var3, var1, method="pearson")

# Using lm function
c <- lm(var3 ~ var1)
summary(c)
sqrt(summary(c)$r.squared) # or simply sqrt(0.4856)
```

The `summary()` of the `lm()` function actually prints out the R-squared value,
which is the % variation explained, rather than the correlation coefficient. To
obtain the correlation coefficient simply run the `sqrt()` function on the R-
squared value output from the summary, e.g. `sqrt(0.4856)` in this specific 
example, or the slightly longer `sqrt(summary(c)$r.squared)` where `c` is the
results of your `lm()` function for a more general approach.

Play around with the code in the above exercise; you will notice that the 
correlation coefficient, r, is unchanged irrespective of whether var3 or var1 is
first in the `lm()` function call.

### Rank correlation
Again, if your data are very highly skewed you might want to try a non-parametric
method, although sometimes it is better to transform your data (e.g. with logarithms).
Rank correlation is usually referred to as **Spearman** rank correlation. Again,
you can do it with the built-in R command, or the `lm()` function. For the `lm()`
function we simply have to `rank()` the values from lowest to highest first. Rank
correlation is referred to by the Greek letter rho, shown as $\rho$ .

```{r, rank_correlation, exercise = TRUE}
# Built-in method
cor.test(var3, var1, method="spearman")

# lm
b <- lm(rank(var3) ~ rank(var1))
summary(b)
sqrt(summary(b)$r.squared) # Or simply look at the summary output and sqrt(0.4964)
```

In general try to avoid non-parametric tests when possible, due to their lower
statistical power.

## Ony-way ANOVA, two-way ANOVA, ANCOVA
Actually, you've already done these using `lm()` which in my opinion is definitely
the easiest way of doing the analyses.  Just to recap, in all three cases the
response is a continuous variable, and the explanatories are:

* One-way ANOVA. `lm()` One explanatory factor (categorical variable) with 3 or more levels
* Two-way ANOVA. `lm()` Two explanatory factors (categorical variables) plus their interaction
* ANCOVA `lm()` with two explanatory variables, one a factor (categorical), the other
a continuous variable (sometimes called a covariate) plus the interactions.

You will find that for Two-way ANOVA with unbalanced data, and for ANCOVA the most
accurate F-values are by using the `Anova()` function from the `car` package which
does Type-III Sums of Squares.